---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(dplyr)
```


```{r}
setwd("C:/Users/jesse/Desktop/Rprojects2024/MachineLearningWithR/abalone")
abalone_data <- read.table("./data/abalone.data", header = FALSE, sep = ",")
colnames(abalone_data) <- c("sex", "length", "diameter", "height", "wholeWeight", "shuckedWeight", "viceraWeight", "shellWeight", "rings")
```


isolate 75% of the data for training, and the remaining 25% for testing.
```{r}

testing_data_with_age <- abalone_data[sample(
  nrow(abalone_data), floor(nrow(abalone_data)*0.25)),]


training_data <- anti_join(abalone_data, testing_data_with_age, by=c("sex", "length", "diameter", "height", "wholeWeight", "shuckedWeight", "viceraWeight", "shellWeight", "rings"))

nrow(testing_data_with_age)+nrow(training_data)==nrow(abalone_data)

```


Check for any missing values before training
```{r}
missing_training_count <- sum(is.na(training_data))
missing_testing_count <- sum(is.na(testing_data_with_age))
cat("missing training count: ", missing_training_count)
cat("\n")
cat("missing testing count: ", missing_testing_count)
```

Normalize the data for training
```{r}
#convert to numeric data for every column
training_data_numeric <- training_data
training_data_numeric$sex <- as.numeric(factor(training_data_numeric$sex))

testing_data_numeric_with_age <- testing_data_with_age
testing_data_numeric_with_age$sex <- as.numeric(factor(testing_data_numeric_with_age$sex))

training_data_numeric <- sapply(training_data_numeric, as.numeric, na.rm=TRUE)
testing_data_numeric_with_age <- sapply(testing_data_numeric_with_age, as.numeric, na.rm=TRUE)

tail(training_data_numeric,5)
tail(testing_data_numeric_with_age, 5)

#normalize the data
normalized_training_data <- scale(training_data_numeric[, -ncol(training_data_numeric)])
normalized_testing_data_with_age <- scale(testing_data_numeric_with_age[, -ncol(testing_data_numeric_with_age)])

tail(training_data_numeric,5)
tail(testing_data_numeric_with_age, 5)
```
https://www.stratascratch.com/blog/machine-learning-in-r-for-beginners-super-simple-way-to-start/

Linear regression model

```{r}
y_training <- training_data_numeric[, ncol(training_data_numeric), drop=FALSE]
x_training <- training_data_numeric[, -ncol(training_data_numeric)]

y_testing <- testing_data_numeric_with_age[, ncol(testing_data_numeric_with_age), drop=FALSE]
x_testing <- testing_data_numeric_with_age[, -ncol(testing_data_numeric_with_age)]

print(head(x_training, 5))

lm_model <- lm(y_training ~ sex + length + diameter + height + wholeWeight + shuckedWeight + viceraWeight + shellWeight, data=as.data.frame(x_training))
summary(lm_model)

predictions <- predict(lm_model, newdata = as.data.frame(x_testing))

mean_squared_error <- mean((predictions - y_testing) ^ 2)

print(paste("mean squared error on the test set is : ", mean_squared_error))

```


```{r}
plot(abalone_data$length, abalone_data$rings,
     main="length vs rings, abalone",
     xlab="length",
     ylab="rings")

plot(abalone_data$wholeWeight, abalone_data$rings,
     main="whole weight vs rings, abalone",
     xlab="whole weight",
     ylab="rings")

plot(abalone_data$diameter, abalone_data$rings,
     main="diameter vs rings, abalone",
     xlab="diameter",
     ylab="rings")


plot(abalone_data$height, abalone_data$rings,
     main="height vs rings, abalone",
     xlab="height",
     ylab="rings")

plot(abalone_data$shuckedWeight, abalone_data$rings,
     main="shucked weight vs rings, abalone",
     xlab="shucked weight",
     ylab="rings")

plot(abalone_data$viceraWeight, abalone_data$rings,
     main="vicera weight vs rings, abalone",
     xlab="vicera weight",
     ylab="rings")

plot(abalone_data$shellWeight, abalone_data$rings,
     main="shell weight vs rings, abalone",
     xlab="shell weight",
     ylab="rings")

barplot(abalone_data$rings,
     main="sex rings, abalone",
     names=abalone_data$sex,
     xlab="sex",
     ylab="rings")
```

Seems like the weights are logarithmic and the dimensions are linear. Lets try another linear model on just the dimensions


```{r}
y_training <- training_data_numeric[, ncol(training_data_numeric), drop=FALSE]
x_training <- training_data_numeric[, -ncol(training_data_numeric)]

y_testing <- testing_data_numeric_with_age[, ncol(testing_data_numeric_with_age), drop=FALSE]
x_testing <- testing_data_numeric_with_age[, -ncol(testing_data_numeric_with_age)]

print(head(x_training, 5))

lm_model <- lm(y_training ~ sex + length + diameter + height, data=as.data.frame(x_training))
summary(lm_model)

predictions <- predict(lm_model, newdata = as.data.frame(x_testing))

mean_squared_error <- mean((predictions - y_testing) ^ 2)

print(paste("mean squared error on the test set is : ", mean_squared_error))

```

It seems to perform considerably worse - lol

===========================
Now we are switching from Linear regression to Decision Tree

```{r}
library(tree)

training_data_frame <- as.data.frame(training_data_numeric)
# Build the regression tree model
tree_model <- tree(
  formula=rings ~ sex + length + diameter + height + wholeWeight + shuckedWeight + viceraWeight + shellWeight,
  data=training_data_frame)

# Display information about the tree
summary(tree_model)

# tree(formula, data, weights, subset,
#      na.action = na.pass, control = tree.control(nobs, ...),
#      method = "recursive.partition",
#      split = c("deviance", "gini"),
#      model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...)

```



```{r}
# Plot the tree
plot(tree_model)
text(tree_model, pretty = 0)

testing_data_frame <- as.data.frame(testing_data_numeric_with_age)
# Predict using the regression tree
predictions_tree <- predict(tree_model, newdata = testing_data_frame)

# Optional: Evaluate the performance of the model with Mean Squared Error
y_test <- testing_data_frame$rings  # Assuming 'target' is the actual response variable in your test set
mse <- mean((predictions_tree - y_test)^2)

# Print the Mean Squared Error
print(paste("Mean Squared Error:", mse))
```
Its a bit funny to apply a decision tree style solution to something that has a continuous type of result. I doubt the practicality of this.


To the future! A neural Net

```{r}
library(nnet)

# Training a simple neural network model
nn_model <- nnet(rings ~ ., data = training_data_frame, size = 5, linout = TRUE)
summary(nn_model)

# Prediction
nn_predictions <- predict(nn_model, newdata = testing_data_frame, type = "raw")
mse_nn <- mean((nn_predictions - testing_data_frame$rings)^2)
print(paste("MSE:", mse_nn))
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
